{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import neuron\n",
    "import linear\n",
    "import time\n",
    "# time-to-first-spike\n",
    "from ttfe import latency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# modify these values if you use other datasets\n",
    "input_dimmension = 784                 # input feature의 개수 (기존 MNIST에서는  784)\n",
    "hidden_dimmension = 100                # hidden layer 개수 (기존 MNIST에서는 100)\n",
    "output_dimmension = 10                # output label 개수 (기존 MNIST에서는 10)\n",
    "batch_sz = 128                          # batch size : dataset의 크기에 따라 조절 (기존 MNIST에서는 128)\n",
    "data_path = './WineQT.csv'            # dataset이 저장되어있는 경로 // dataset이 csv파일이고 마지막 feature가 output label이라고 가정\n",
    "\n",
    "\n",
    "## 임의의 dataset을 사용하기 위해 만든 class\n",
    "class SNN_Dataset(Dataset):\n",
    "  def __init__(self, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    self.x_data = df.iloc[:, :-1].values\n",
    "    self.y_data = df.iloc[:, -1].values\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "  def __getitem__(self, idx):\n",
    "    return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearly all paramters in this simulation, but it seems no use to store them in a dict...\n",
    "param = {'G_min': -1.0, # the minimum (maximum) value of conductance, I found if I use real conductance value,#-1.0\n",
    "        'G_max': 1.0,   # which is on 1e-4 level, the gradience will be too small even in two layer net.#1.0\n",
    "        'Rd': 10e9,    # this device resistance is mannually set for smaller leaky current? / 5.0e9\n",
    "        'Cm': 80e-12,   # real capacitance is absolutely larger than this value / 0.8e-10\n",
    "        'Rs': 1005000,      # this series resistance value is mannually set for larger inject current?\n",
    "        'Vth': 4.2,     # this is the real device threshould voltage #5.6\n",
    "        'V_reset': 3.7,\n",
    "        'dt': 1.75e-4,   # every time step is dt, in the one-order differential equation of neuron\n",
    "        'T_sim': 10,   # could control total spike number collected\n",
    "        'dim_in': input_dimmension,\n",
    "        'dim_h': hidden_dimmension,\n",
    "        'dim_out': output_dimmension,\n",
    "        'amp' : 3,    # the gain of TIAs\n",
    "        'q_bit': 7,     # quantize bit\n",
    "        'epoch': 100,\n",
    "        'batch_size': batch_sz,#\n",
    "        'learning_rate': 0.0135, #I am not sure if this lr is too large  0.012\n",
    "        'data_dir': './MNIST',\n",
    "        'train_file': 'trainning_log_7bit.txt',\n",
    "        'test_file': 'test_log.txt',\n",
    "        'model_dir': 'Model.pth'\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "def rate_encoding(x, T_sim):\n",
    "    '''\n",
    "    Encodes the input image pixels into periodic spike trains.\n",
    "    Spike rate is proportional to pixel value, and spikes occur at regular intervals.\n",
    "    \n",
    "    input: x (batch_size, dim_in) - input data\n",
    "           T_sim (int) - number of time steps for the simulation\n",
    "    output: (batch_size, dim_in, T_sim) - periodic spikes for each pixel over T_sim time steps\n",
    "    '''\n",
    "    batch_size, dim_in = x.shape\n",
    "    spikes = torch.zeros(batch_size, dim_in, T_sim).to(x.device)  # Initialize spikes tensor\n",
    "\n",
    "    # Determine the spike period based on pixel value (pixel value is normalized between 0 and 1)\n",
    "    rate = x - 1e-8  # Add a small value to avoid division by zero\n",
    "    # Generate spikes at regular intervals based on period\n",
    "    for t in range(T_sim):\n",
    "        spikes[:, :, t] =  ((t+1)* rate -  ((t + 1)* rate)).int() < (t*rate - (t * rate).int()) # Spike occurs when time step modulo period is 0\n",
    "\n",
    "    return spikes\n",
    "\"\"\"\n",
    "\n",
    "def rate_encoding(x, T_sim):\n",
    "    '''\n",
    "    Encodes the input image pixels into periodic spike trains.\n",
    "    Spike rate is proportional to pixel value, and spikes occur at regular intervals.\n",
    "    \n",
    "    input: x (batch_size, dim_in) - input data\n",
    "           T_sim (int) - number of time steps for the simulation\n",
    "    output: (batch_size, dim_in, T_sim) - periodic spikes for each pixel over T_sim time steps\n",
    "    '''\n",
    "    batch_size, dim_in = x.shape\n",
    "    spikes = torch.zeros(batch_size, dim_in, T_sim).to(x.device)  # Initialize spikes tensor\n",
    "\n",
    "    # Define the (pixel intensity, period) pairs\n",
    "    pixel_values = torch.tensor([0, 0.287273, 0.389091, 0.592727, 0.796364, 1]).to(x.device)\n",
    "    periods = torch.tensor([6.5, 3.1, 2.6, 2.2, 1.9, 1.7]).to(x.device)\n",
    "\n",
    "    # Use torch's linear interpolation (lerp) to compute periods based on pixel values\n",
    "    period = torch.zeros_like(x)  # Initialize period tensor with the same shape as x\n",
    "    for i in range(len(pixel_values) - 1):\n",
    "        # Find pixels that are in the current pixel intensity range\n",
    "        mask = (x >= pixel_values[i]) & (x < pixel_values[i+1])\n",
    "        \n",
    "        # Linearly interpolate periods for those pixels\n",
    "        period[mask] = periods[i] + (periods[i+1] - periods[i]) * (x[mask] - pixel_values[i]) / (pixel_values[i+1] - pixel_values[i])\n",
    "\n",
    "    # Ensure pixel intensity == 1 gets the period of 1.7\n",
    "    period[x == 1] = 1.7\n",
    "\n",
    "    # Generate spikes at regular intervals based on the calculated period\n",
    "    for t in range(T_sim):\n",
    "        # Spike occurs when t modulo period is approximately 0\n",
    "        #spikes[:, :, t] = ((t % period) < 1).float()\n",
    "        spikes[:, :, t] = ((t+1) // period > t // period)\n",
    "    return spikes\n",
    "\n",
    "def Poisson_encoder(x):\n",
    "    '''\n",
    "    To encode the image pixels to poisson event.\n",
    "\n",
    "    input: a batch of input data x.\n",
    "    output: a batch of poisson encoded 1.0 or 0.0 with the same shape as x,\n",
    "            the possibility of a pixle to be encoded as 1.0 is propotional to the pixel value.\n",
    "    '''\n",
    "    out_spike = torch.rand_like(x).le(x).float()\n",
    "    return out_spike\n",
    "\n",
    "\n",
    "def ttfs_encoding(x, T_sim):\n",
    "    '''\n",
    "    Encodes the input image pixels using time-to-first-spike encoding based on provided data.\n",
    "    Spike time is inversely proportional to pixel value; higher pixel values result in earlier spikes.\n",
    "    The first spike occurs at time step 0, and the latest spike occurs at time step T_sim.\n",
    "\n",
    "    input: x (batch_size, dim_in) - input data\n",
    "           T_sim (int) - number of time steps for the simulation\n",
    "           dt (float) - time step duration in ms (default is 0.1 ms)\n",
    "    output: (batch_size, dim_in, T_sim) - time-to-first-spike encoded spikes\n",
    "    '''\n",
    "    \n",
    "    batch_size, dim_in = x.shape\n",
    "    spikes = torch.zeros(batch_size, dim_in, T_sim).to(x.device)  # Initialize spikes tensor\n",
    "\n",
    "    # Define the (pixel intensity, spike time in ms) pairs for interpolation\n",
    "    pixel_values = torch.tensor([0, 0.287273, 0.389091, 0.592727, 0.796364, 1]).to(x.device)\n",
    "    spike_times = torch.tensor([14.3, 12.4, 11.8, 11.4, 11.3, 11.2]).to(x.device)  # Spike times in ms\n",
    "    spike_times -= 11.2\n",
    "    # Convert spike times from ms to time steps based on the time step duration (dt)\n",
    "    spike_times = spike_times * 1  # Convert ms to time steps (e.g., 14.3 ms / 0.1 ms per step = 143 steps)\n",
    "    spike_time = torch.zeros_like(x)  # Initialize spike time tensor with the same shape as x\n",
    "    \n",
    "    # Initialize spike time tensor\n",
    "\n",
    "    # Linearly interpolate spike times based on pixel values\n",
    "    for i in range(len(pixel_values) - 1):\n",
    "        # Find pixels that are in the current pixel intensity range\n",
    "        mask = (x >= pixel_values[i]) & (x < pixel_values[i+1])\n",
    "        \n",
    "        # Linearly interpolate spike times for those pixels\n",
    "        spike_time[mask] = spike_times[i] + (spike_times[i+1] - spike_times[i]) * (x[mask] - pixel_values[i]) / (pixel_values[i+1] - pixel_values[i])\n",
    "\n",
    "    # Ensure pixel intensity == 1 gets the spike time of 11.2 ms (converted to time steps)\n",
    "    spike_time[x == 1] = spike_times[-1]\n",
    "\n",
    "\n",
    "    # Generate spikes based on the calculated spike times (time-to-first-spike encoding)\n",
    "    for t in range(T_sim):\n",
    "        # Spike occurs when the time step equals the spike time\n",
    "        spikes[:, :, t] = (spike_time < t + 1).float() * (spike_time >= t).float()\n",
    "\n",
    "    return spikes\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Three_Layer_SNN(nn.Module):\n",
    "    '''\n",
    "    This net model contains 2 linear layer, 2 self-defined BatchNorm layer and 2 Neuron layer.\n",
    "\n",
    "    linear layer: a memristor crossbar on which the MAC operation is implemented.\n",
    "    BatchNorm layer: a row of TIA as the output interface of the pre-linear layer, normalize the\n",
    "                    output current to  -2.0~2.0 V voltage.\n",
    "    neuron layer: nonliear activation, receive input voltage and output spikes, spiking rate is taken\n",
    "                    in loss computing.\n",
    "    '''\n",
    "    def __init__(self, param):\n",
    "        super().__init__()\n",
    "        self.linear1 = linear.MAC_Crossbar(param['dim_in'], param['dim_h'],\n",
    "                                            param['G_min'], param['G_max'], param['q_bit'])\n",
    "        self.BatchNorm1 = linear.TIA_Norm(param['dim_in'], 0.0, 200.0)    # the paramters of TIA are mannually set for moderate input voltage to neurons\n",
    "        self.neuron1 = neuron.LIFNeuron(param['batch_size'], param['dim_h'], param['Rd'], param['Cm'],\n",
    "                                            param['Rs'], param['Vth'], param['V_reset'], param['dt'])\n",
    "        self.linear2 = linear.MAC_Crossbar(param['dim_h'], param['dim_out'],\n",
    "                                            param['G_min'], param['G_max'], param['q_bit'])\n",
    "        self.BatchNorm2 = linear.TIA_Norm(param['dim_h'], 0.0, 200.0)    # same as above\n",
    "        self.neuron2 = neuron.LIFNeuron(param['batch_size'], param['dim_out'], param['Rd'], param['Cm'],\n",
    "                                            param['Rs'], param['Vth'], param['V_reset'], param['dt'])\n",
    "\n",
    "    def forward(self, input_vector):\n",
    "        out_vector = self.linear1(input_vector)\n",
    "\n",
    "        # debug print, very useful to see what happend in every layer\n",
    "        #print('0', out_vector.max())\n",
    "        out_vector = self.BatchNorm1(out_vector)\n",
    "        #print('1', out_vector.max())\n",
    "\n",
    "        ## debug\n",
    "        ##print(type(input_vector), type(out_vector))\n",
    "        ##print(input_vector.is_cuda, out_vector.is_cuda)\n",
    "        self.neuron1.v = self.neuron1.v.to(device)\n",
    "        self.neuron2.v = self.neuron2.v.to(device)\n",
    "        ##print(type(self.neuron1.v), self.neuron1.v.device)\n",
    "        ##\n",
    "\n",
    "        out_vector = self.neuron1(out_vector)\n",
    "\n",
    "        #print('2', out_vector.sum(1).max())\n",
    "        out_vector = self.linear2(out_vector)\n",
    "        #print('3', out_vector.max())\n",
    "        out_vector = self.BatchNorm2(out_vector)\n",
    "        #print('4', out_vector.max())\n",
    "        out_vector = self.neuron2(out_vector)\n",
    "        #print('5', out_vector.sum(1).max())\n",
    "        return out_vector\n",
    "\n",
    "    def reset_(self):\n",
    "        '''\n",
    "        Reset all neurons after one forward pass,\n",
    "        to ensure the independency of every input image.\n",
    "        '''\n",
    "        for item in self.modules():\n",
    "            if hasattr(item, 'reset'):\n",
    "                item.reset()\n",
    "\n",
    "    def quant_(self):\n",
    "        '''\n",
    "        The quantization function in pytorch only support int8,\n",
    "        so we need our own quant function for adjustable quantization precision.\n",
    "        '''\n",
    "        for item in self.modules():\n",
    "            if hasattr(item, 'Gquant_'):\n",
    "                #debug print：\n",
    "                #print(item.weight.max())\n",
    "                item.Gquant_()\n",
    "                #debug print：\n",
    "                #print(item.weight.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Values: tensor([[0.0000, 0.0101, 0.0202, 0.0303, 0.0404, 0.0505, 0.0606, 0.0707, 0.0808,\n",
      "         0.0909, 0.1010, 0.1111, 0.1212, 0.1313, 0.1414, 0.1515, 0.1616, 0.1717,\n",
      "         0.1818, 0.1919, 0.2020, 0.2121, 0.2222, 0.2323, 0.2424, 0.2525, 0.2626,\n",
      "         0.2727, 0.2828, 0.2929, 0.3030, 0.3131, 0.3232, 0.3333, 0.3434, 0.3535,\n",
      "         0.3636, 0.3737, 0.3838, 0.3939, 0.4040, 0.4141, 0.4242, 0.4343, 0.4444,\n",
      "         0.4545, 0.4646, 0.4747, 0.4848, 0.4949, 0.5051, 0.5152, 0.5253, 0.5354,\n",
      "         0.5455, 0.5556, 0.5657, 0.5758, 0.5859, 0.5960, 0.6061, 0.6162, 0.6263,\n",
      "         0.6364, 0.6465, 0.6566, 0.6667, 0.6768, 0.6869, 0.6970, 0.7071, 0.7172,\n",
      "         0.7273, 0.7374, 0.7475, 0.7576, 0.7677, 0.7778, 0.7879, 0.7980, 0.8081,\n",
      "         0.8182, 0.8283, 0.8384, 0.8485, 0.8586, 0.8687, 0.8788, 0.8889, 0.8990,\n",
      "         0.9091, 0.9192, 0.9293, 0.9394, 0.9495, 0.9596, 0.9697, 0.9798, 0.9899,\n",
      "         1.0000]])\n",
      "\n",
      " original ttfe (converted to first spike format):\n",
      "Pixel 0.0: Spikes at time step(s) []\n",
      "Pixel 0.010101010091602802: Spikes at time step(s) []\n",
      "Pixel 0.020202020183205605: Spikes at time step(s) [7]\n",
      "Pixel 0.03030303120613098: Spikes at time step(s) [4]\n",
      "Pixel 0.04040404036641121: Spikes at time step(s) [3]\n",
      "Pixel 0.05050504952669144: Spikes at time step(s) [2]\n",
      "Pixel 0.06060606241226196: Spikes at time step(s) [2]\n",
      "Pixel 0.07070706784725189: Spikes at time step(s) [2]\n",
      "Pixel 0.08080808073282242: Spikes at time step(s) [1]\n",
      "Pixel 0.09090909361839294: Spikes at time step(s) [1]\n",
      "Pixel 0.10101009905338287: Spikes at time step(s) [1]\n",
      "Pixel 0.1111111119389534: Spikes at time step(s) [1]\n",
      "Pixel 0.12121212482452393: Spikes at time step(s) [1]\n",
      "Pixel 0.13131313025951385: Spikes at time step(s) [1]\n",
      "Pixel 0.14141413569450378: Spikes at time step(s) [1]\n",
      "Pixel 0.1515151560306549: Spikes at time step(s) [1]\n",
      "Pixel 0.16161616146564484: Spikes at time step(s) [1]\n",
      "Pixel 0.17171716690063477: Spikes at time step(s) [1]\n",
      "Pixel 0.1818181872367859: Spikes at time step(s) [1]\n",
      "Pixel 0.19191919267177582: Spikes at time step(s) [1]\n",
      "Pixel 0.20202019810676575: Spikes at time step(s) [1]\n",
      "Pixel 0.21212121844291687: Spikes at time step(s) [0]\n",
      "Pixel 0.2222222238779068: Spikes at time step(s) [0]\n",
      "Pixel 0.23232322931289673: Spikes at time step(s) [0]\n",
      "Pixel 0.24242424964904785: Spikes at time step(s) [0]\n",
      "Pixel 0.2525252401828766: Spikes at time step(s) [0]\n",
      "Pixel 0.2626262605190277: Spikes at time step(s) [0]\n",
      "Pixel 0.27272728085517883: Spikes at time step(s) [0]\n",
      "Pixel 0.28282827138900757: Spikes at time step(s) [0]\n",
      "Pixel 0.2929292917251587: Spikes at time step(s) [0]\n",
      "Pixel 0.3030303120613098: Spikes at time step(s) [0]\n",
      "Pixel 0.31313130259513855: Spikes at time step(s) [0]\n",
      "Pixel 0.3232323229312897: Spikes at time step(s) [0]\n",
      "Pixel 0.3333333432674408: Spikes at time step(s) [0]\n",
      "Pixel 0.34343433380126953: Spikes at time step(s) [0]\n",
      "Pixel 0.35353535413742065: Spikes at time step(s) [0]\n",
      "Pixel 0.3636363744735718: Spikes at time step(s) [0]\n",
      "Pixel 0.3737373650074005: Spikes at time step(s) [0]\n",
      "Pixel 0.38383838534355164: Spikes at time step(s) [0]\n",
      "Pixel 0.39393940567970276: Spikes at time step(s) [0]\n",
      "Pixel 0.4040403962135315: Spikes at time step(s) [0]\n",
      "Pixel 0.4141414165496826: Spikes at time step(s) [0]\n",
      "Pixel 0.42424243688583374: Spikes at time step(s) [0]\n",
      "Pixel 0.4343434274196625: Spikes at time step(s) [0]\n",
      "Pixel 0.4444444477558136: Spikes at time step(s) [0]\n",
      "Pixel 0.4545454680919647: Spikes at time step(s) [0]\n",
      "Pixel 0.46464645862579346: Spikes at time step(s) [0]\n",
      "Pixel 0.4747474789619446: Spikes at time step(s) [0]\n",
      "Pixel 0.4848484992980957: Spikes at time step(s) [0]\n",
      "Pixel 0.49494948983192444: Spikes at time step(s) [0]\n",
      "Pixel 0.5050504803657532: Spikes at time step(s) [0]\n",
      "Pixel 0.5151515007019043: Spikes at time step(s) [0]\n",
      "Pixel 0.5252525210380554: Spikes at time step(s) [0]\n",
      "Pixel 0.5353535413742065: Spikes at time step(s) [0]\n",
      "Pixel 0.5454545617103577: Spikes at time step(s) [0]\n",
      "Pixel 0.5555555820465088: Spikes at time step(s) [0]\n",
      "Pixel 0.5656565427780151: Spikes at time step(s) [0]\n",
      "Pixel 0.5757575631141663: Spikes at time step(s) [0]\n",
      "Pixel 0.5858585834503174: Spikes at time step(s) [0]\n",
      "Pixel 0.5959596037864685: Spikes at time step(s) [0]\n",
      "Pixel 0.6060606241226196: Spikes at time step(s) [0]\n",
      "Pixel 0.6161616444587708: Spikes at time step(s) [0]\n",
      "Pixel 0.6262626051902771: Spikes at time step(s) [0]\n",
      "Pixel 0.6363636255264282: Spikes at time step(s) [0]\n",
      "Pixel 0.6464646458625793: Spikes at time step(s) [0]\n",
      "Pixel 0.6565656661987305: Spikes at time step(s) [0]\n",
      "Pixel 0.6666666865348816: Spikes at time step(s) [0]\n",
      "Pixel 0.6767677068710327: Spikes at time step(s) [0]\n",
      "Pixel 0.6868686676025391: Spikes at time step(s) [0]\n",
      "Pixel 0.6969696879386902: Spikes at time step(s) [0]\n",
      "Pixel 0.7070707082748413: Spikes at time step(s) [0]\n",
      "Pixel 0.7171717286109924: Spikes at time step(s) [0]\n",
      "Pixel 0.7272727489471436: Spikes at time step(s) [0]\n",
      "Pixel 0.7373737096786499: Spikes at time step(s) [0]\n",
      "Pixel 0.747474730014801: Spikes at time step(s) [0]\n",
      "Pixel 0.7575757503509521: Spikes at time step(s) [0]\n",
      "Pixel 0.7676767706871033: Spikes at time step(s) [0]\n",
      "Pixel 0.7777777910232544: Spikes at time step(s) [0]\n",
      "Pixel 0.7878788113594055: Spikes at time step(s) [0]\n",
      "Pixel 0.7979797720909119: Spikes at time step(s) [0]\n",
      "Pixel 0.808080792427063: Spikes at time step(s) [0]\n",
      "Pixel 0.8181818127632141: Spikes at time step(s) [0]\n",
      "Pixel 0.8282828330993652: Spikes at time step(s) [0]\n",
      "Pixel 0.8383838534355164: Spikes at time step(s) [0]\n",
      "Pixel 0.8484848737716675: Spikes at time step(s) [0]\n",
      "Pixel 0.8585858345031738: Spikes at time step(s) [0]\n",
      "Pixel 0.868686854839325: Spikes at time step(s) [0]\n",
      "Pixel 0.8787878751754761: Spikes at time step(s) [0]\n",
      "Pixel 0.8888888955116272: Spikes at time step(s) [0]\n",
      "Pixel 0.8989899158477783: Spikes at time step(s) [0]\n",
      "Pixel 0.9090909361839294: Spikes at time step(s) [0]\n",
      "Pixel 0.9191918969154358: Spikes at time step(s) [0]\n",
      "Pixel 0.9292929172515869: Spikes at time step(s) [0]\n",
      "Pixel 0.939393937587738: Spikes at time step(s) [0]\n",
      "Pixel 0.9494949579238892: Spikes at time step(s) [0]\n",
      "Pixel 0.9595959782600403: Spikes at time step(s) [0]\n",
      "Pixel 0.9696969985961914: Spikes at time step(s) [0]\n",
      "Pixel 0.9797979593276978: Spikes at time step(s) [0]\n",
      "Pixel 0.9898989796638489: Spikes at time step(s) [0]\n",
      "Pixel 1.0: Spikes at time step(s) [0]\n",
      "\n",
      " new ttfe :\n",
      "Pixel 0.0: Spikes at time step(s) [3]\n",
      "Pixel 0.010101010091602802: Spikes at time step(s) [3]\n",
      "Pixel 0.020202020183205605: Spikes at time step(s) [2]\n",
      "Pixel 0.03030303120613098: Spikes at time step(s) [2]\n",
      "Pixel 0.04040404036641121: Spikes at time step(s) [2]\n",
      "Pixel 0.05050504952669144: Spikes at time step(s) [2]\n",
      "Pixel 0.06060606241226196: Spikes at time step(s) [2]\n",
      "Pixel 0.07070706784725189: Spikes at time step(s) [2]\n",
      "Pixel 0.08080808073282242: Spikes at time step(s) [2]\n",
      "Pixel 0.09090909361839294: Spikes at time step(s) [2]\n",
      "Pixel 0.10101009905338287: Spikes at time step(s) [2]\n",
      "Pixel 0.1111111119389534: Spikes at time step(s) [2]\n",
      "Pixel 0.12121212482452393: Spikes at time step(s) [2]\n",
      "Pixel 0.13131313025951385: Spikes at time step(s) [2]\n",
      "Pixel 0.14141413569450378: Spikes at time step(s) [2]\n",
      "Pixel 0.1515151560306549: Spikes at time step(s) [2]\n",
      "Pixel 0.16161616146564484: Spikes at time step(s) [2]\n",
      "Pixel 0.17171716690063477: Spikes at time step(s) [1]\n",
      "Pixel 0.1818181872367859: Spikes at time step(s) [1]\n",
      "Pixel 0.19191919267177582: Spikes at time step(s) [1]\n",
      "Pixel 0.20202019810676575: Spikes at time step(s) [1]\n",
      "Pixel 0.21212121844291687: Spikes at time step(s) [1]\n",
      "Pixel 0.2222222238779068: Spikes at time step(s) [1]\n",
      "Pixel 0.23232322931289673: Spikes at time step(s) [1]\n",
      "Pixel 0.24242424964904785: Spikes at time step(s) [1]\n",
      "Pixel 0.2525252401828766: Spikes at time step(s) [1]\n",
      "Pixel 0.2626262605190277: Spikes at time step(s) [1]\n",
      "Pixel 0.27272728085517883: Spikes at time step(s) [1]\n",
      "Pixel 0.28282827138900757: Spikes at time step(s) [1]\n",
      "Pixel 0.2929292917251587: Spikes at time step(s) [1]\n",
      "Pixel 0.3030303120613098: Spikes at time step(s) [1]\n",
      "Pixel 0.31313130259513855: Spikes at time step(s) [1]\n",
      "Pixel 0.3232323229312897: Spikes at time step(s) [0]\n",
      "Pixel 0.3333333432674408: Spikes at time step(s) [0]\n",
      "Pixel 0.34343433380126953: Spikes at time step(s) [0]\n",
      "Pixel 0.35353535413742065: Spikes at time step(s) [0]\n",
      "Pixel 0.3636363744735718: Spikes at time step(s) [0]\n",
      "Pixel 0.3737373650074005: Spikes at time step(s) [0]\n",
      "Pixel 0.38383838534355164: Spikes at time step(s) [0]\n",
      "Pixel 0.39393940567970276: Spikes at time step(s) [0]\n",
      "Pixel 0.4040403962135315: Spikes at time step(s) [0]\n",
      "Pixel 0.4141414165496826: Spikes at time step(s) [0]\n",
      "Pixel 0.42424243688583374: Spikes at time step(s) [0]\n",
      "Pixel 0.4343434274196625: Spikes at time step(s) [0]\n",
      "Pixel 0.4444444477558136: Spikes at time step(s) [0]\n",
      "Pixel 0.4545454680919647: Spikes at time step(s) [0]\n",
      "Pixel 0.46464645862579346: Spikes at time step(s) [0]\n",
      "Pixel 0.4747474789619446: Spikes at time step(s) [0]\n",
      "Pixel 0.4848484992980957: Spikes at time step(s) [0]\n",
      "Pixel 0.49494948983192444: Spikes at time step(s) [0]\n",
      "Pixel 0.5050504803657532: Spikes at time step(s) [0]\n",
      "Pixel 0.5151515007019043: Spikes at time step(s) [0]\n",
      "Pixel 0.5252525210380554: Spikes at time step(s) [0]\n",
      "Pixel 0.5353535413742065: Spikes at time step(s) [0]\n",
      "Pixel 0.5454545617103577: Spikes at time step(s) [0]\n",
      "Pixel 0.5555555820465088: Spikes at time step(s) [0]\n",
      "Pixel 0.5656565427780151: Spikes at time step(s) [0]\n",
      "Pixel 0.5757575631141663: Spikes at time step(s) [0]\n",
      "Pixel 0.5858585834503174: Spikes at time step(s) [0]\n",
      "Pixel 0.5959596037864685: Spikes at time step(s) [0]\n",
      "Pixel 0.6060606241226196: Spikes at time step(s) [0]\n",
      "Pixel 0.6161616444587708: Spikes at time step(s) [0]\n",
      "Pixel 0.6262626051902771: Spikes at time step(s) [0]\n",
      "Pixel 0.6363636255264282: Spikes at time step(s) [0]\n",
      "Pixel 0.6464646458625793: Spikes at time step(s) [0]\n",
      "Pixel 0.6565656661987305: Spikes at time step(s) [0]\n",
      "Pixel 0.6666666865348816: Spikes at time step(s) [0]\n",
      "Pixel 0.6767677068710327: Spikes at time step(s) [0]\n",
      "Pixel 0.6868686676025391: Spikes at time step(s) [0]\n",
      "Pixel 0.6969696879386902: Spikes at time step(s) [0]\n",
      "Pixel 0.7070707082748413: Spikes at time step(s) [0]\n",
      "Pixel 0.7171717286109924: Spikes at time step(s) [0]\n",
      "Pixel 0.7272727489471436: Spikes at time step(s) [0]\n",
      "Pixel 0.7373737096786499: Spikes at time step(s) [0]\n",
      "Pixel 0.747474730014801: Spikes at time step(s) [0]\n",
      "Pixel 0.7575757503509521: Spikes at time step(s) [0]\n",
      "Pixel 0.7676767706871033: Spikes at time step(s) [0]\n",
      "Pixel 0.7777777910232544: Spikes at time step(s) [0]\n",
      "Pixel 0.7878788113594055: Spikes at time step(s) [0]\n",
      "Pixel 0.7979797720909119: Spikes at time step(s) [0]\n",
      "Pixel 0.808080792427063: Spikes at time step(s) [0]\n",
      "Pixel 0.8181818127632141: Spikes at time step(s) [0]\n",
      "Pixel 0.8282828330993652: Spikes at time step(s) [0]\n",
      "Pixel 0.8383838534355164: Spikes at time step(s) [0]\n",
      "Pixel 0.8484848737716675: Spikes at time step(s) [0]\n",
      "Pixel 0.8585858345031738: Spikes at time step(s) [0]\n",
      "Pixel 0.868686854839325: Spikes at time step(s) [0]\n",
      "Pixel 0.8787878751754761: Spikes at time step(s) [0]\n",
      "Pixel 0.8888888955116272: Spikes at time step(s) [0]\n",
      "Pixel 0.8989899158477783: Spikes at time step(s) [0]\n",
      "Pixel 0.9090909361839294: Spikes at time step(s) [0]\n",
      "Pixel 0.9191918969154358: Spikes at time step(s) [0]\n",
      "Pixel 0.9292929172515869: Spikes at time step(s) [0]\n",
      "Pixel 0.939393937587738: Spikes at time step(s) [0]\n",
      "Pixel 0.9494949579238892: Spikes at time step(s) [0]\n",
      "Pixel 0.9595959782600403: Spikes at time step(s) [0]\n",
      "Pixel 0.9696969985961914: Spikes at time step(s) [0]\n",
      "Pixel 0.9797979593276978: Spikes at time step(s) [0]\n",
      "Pixel 0.9898989796638489: Spikes at time step(s) [0]\n",
      "Pixel 1.0: Spikes at time step(s) [0]\n"
     ]
    }
   ],
   "source": [
    "# 픽셀 값 생성 (0 ~ 1 사이 값)\n",
    "pixel_values = torch.linspace(0, 1, 100).reshape(1, -1).to('cpu')  # (batch_size=1, dim_in=100)\n",
    "\n",
    "# 시뮬레이션 파라미터\n",
    "tau = 10\n",
    "threshold = 0.01\n",
    "\n",
    "# 기존 ttfe 방식 (spike_time1)과 새로운 ttfe 방식 (spike_time2)\n",
    "spike_time1 = latency(pixel_values, num_steps=param['T_sim'], tau=tau, threshold=threshold, clip=False, normalize=False, linear=False, bypass=True)\n",
    "\n",
    "# spike_time1의 크기를 (T_sim, 1, N)에서 (1, N, T_sim)로 변경\n",
    "spike_time1 = spike_time1.permute(1, 2, 0)  # (1, N, T_sim) 형식으로 변경\n",
    "\n",
    "spike_time2 = ttfs_encoding(pixel_values, param['T_sim'])\n",
    "\n",
    "# 스파이크가 발생한 time step을 추출하여 spike_time1 형식을 spike_time2 형식으로 변환\n",
    "spike_time1_first_spike = torch.zeros_like(spike_time1)  # 모든 time step에 대한 스파이크 초기화\n",
    "\n",
    "# 각 픽셀에 대해 첫 번째 스파이크가 발생한 time step 찾기\n",
    "for i in range(spike_time1.shape[1]):  # pixel 개수만큼 반복\n",
    "    first_spike_indices = torch.nonzero(spike_time1[0, i, :])  # 첫 번째로 1이 나오는 index를 찾음\n",
    "    if len(first_spike_indices) > 0:\n",
    "        first_spike_time = first_spike_indices[0, 0].item()  # 첫 스파이크 시간 저장\n",
    "        spike_time1_first_spike[0, i, first_spike_time] = 1  # 해당 시점에서만 스파이크 발생\n",
    "\n",
    "# 스파이크 발생 시간 출력\n",
    "print(\"Pixel Values:\", pixel_values)\n",
    "\n",
    "print(\"\\n original ttfe (converted to first spike format):\")\n",
    "for i in range(spike_time1_first_spike.shape[1]):\n",
    "    spike_times = torch.nonzero(spike_time1_first_spike[0, i, :]).flatten()\n",
    "    print(f\"Pixel {pixel_values[0, i].item()}: Spikes at time step(s) {spike_times.tolist()}\")\n",
    "\n",
    "print(\"\\n new ttfe :\")\n",
    "for i in range(spike_time2.shape[1]):\n",
    "    spike_times = torch.nonzero(spike_time2[0, i, :]).flatten()\n",
    "    print(f\"Pixel {pixel_values[0, i].item()}: Spikes at time step(s) {spike_times.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tranning epoch 0: the SNN loss is 1.972365 trainning accuracy: 0.4116 validation accuracy: 0.5444\n",
      "elapsed time : 4.5s\n",
      "tranning epoch 1: the SNN loss is 1.946878 trainning accuracy: 0.5779 validation accuracy: 0.6254\n",
      "elapsed time : 9.0s\n",
      "tranning epoch 2: the SNN loss is 1.960922 trainning accuracy: 0.6290 validation accuracy: 0.6437\n",
      "elapsed time : 13.5s\n",
      "tranning epoch 3: the SNN loss is 1.950788 trainning accuracy: 0.6578 validation accuracy: 0.6775\n",
      "elapsed time : 18.0s\n",
      "tranning epoch 4: the SNN loss is 1.936857 trainning accuracy: 0.6875 validation accuracy: 0.6951\n",
      "elapsed time : 22.4s\n",
      "tranning epoch 5: the SNN loss is 1.932790 trainning accuracy: 0.7010 validation accuracy: 0.7102\n",
      "elapsed time : 26.9s\n",
      "tranning epoch 6: the SNN loss is 1.935008 trainning accuracy: 0.7207 validation accuracy: 0.7354\n",
      "elapsed time : 31.5s\n",
      "tranning epoch 7: the SNN loss is 1.930854 trainning accuracy: 0.7349 validation accuracy: 0.7485\n",
      "elapsed time : 36.0s\n",
      "tranning epoch 8: the SNN loss is 1.931402 trainning accuracy: 0.7393 validation accuracy: 0.7461\n",
      "elapsed time : 40.4s\n",
      "tranning epoch 9: the SNN loss is 1.911047 trainning accuracy: 0.7549 validation accuracy: 0.7738\n",
      "elapsed time : 45.1s\n",
      "tranning epoch 10: the SNN loss is 1.904236 trainning accuracy: 0.7620 validation accuracy: 0.7714\n",
      "elapsed time : 49.4s\n",
      "tranning epoch 11: the SNN loss is 1.942123 trainning accuracy: 0.7735 validation accuracy: 0.7536\n",
      "elapsed time : 54.0s\n",
      "tranning epoch 12: the SNN loss is 1.923919 trainning accuracy: 0.7851 validation accuracy: 0.7868\n",
      "elapsed time : 58.5s\n",
      "tranning epoch 13: the SNN loss is 1.915593 trainning accuracy: 0.7845 validation accuracy: 0.7910\n",
      "elapsed time : 62.8s\n",
      "tranning epoch 14: the SNN loss is 1.924727 trainning accuracy: 0.7936 validation accuracy: 0.8079\n",
      "elapsed time : 67.0s\n",
      "tranning epoch 15: the SNN loss is 1.915279 trainning accuracy: 0.7959 validation accuracy: 0.7985\n",
      "elapsed time : 71.4s\n",
      "tranning epoch 16: the SNN loss is 1.916138 trainning accuracy: 0.8051 validation accuracy: 0.8143\n",
      "elapsed time : 75.6s\n",
      "tranning epoch 17: the SNN loss is 1.920674 trainning accuracy: 0.8073 validation accuracy: 0.8163\n",
      "elapsed time : 79.9s\n",
      "tranning epoch 18: the SNN loss is 1.899867 trainning accuracy: 0.8054 validation accuracy: 0.8232\n",
      "elapsed time : 84.4s\n",
      "tranning epoch 19: the SNN loss is 1.907123 trainning accuracy: 0.8127 validation accuracy: 0.8218\n",
      "elapsed time : 88.9s\n",
      "tranning epoch 20: the SNN loss is 1.903733 trainning accuracy: 0.8113 validation accuracy: 0.8104\n",
      "elapsed time : 93.2s\n",
      "tranning epoch 21: the SNN loss is 1.901668 trainning accuracy: 0.8190 validation accuracy: 0.8297\n",
      "elapsed time : 97.5s\n",
      "tranning epoch 22: the SNN loss is 1.900291 trainning accuracy: 0.8238 validation accuracy: 0.8050\n",
      "elapsed time : 101.7s\n",
      "tranning epoch 23: the SNN loss is 1.915857 trainning accuracy: 0.8255 validation accuracy: 0.8176\n",
      "elapsed time : 106.3s\n",
      "tranning epoch 24: the SNN loss is 1.913325 trainning accuracy: 0.8230 validation accuracy: 0.8672\n",
      "elapsed time : 111.0s\n",
      "tranning epoch 25: the SNN loss is 1.903584 trainning accuracy: 0.8290 validation accuracy: 0.8319\n",
      "elapsed time : 115.3s\n",
      "tranning epoch 26: the SNN loss is 1.901310 trainning accuracy: 0.8340 validation accuracy: 0.8216\n",
      "elapsed time : 119.6s\n",
      "tranning epoch 27: the SNN loss is 1.922443 trainning accuracy: 0.8341 validation accuracy: 0.8652\n",
      "elapsed time : 123.9s\n",
      "tranning epoch 28: the SNN loss is 1.911960 trainning accuracy: 0.8339 validation accuracy: 0.8396\n",
      "elapsed time : 128.2s\n",
      "tranning epoch 29: the SNN loss is 1.891143 trainning accuracy: 0.8388 validation accuracy: 0.8511\n",
      "elapsed time : 132.5s\n",
      "tranning epoch 30: the SNN loss is 1.906229 trainning accuracy: 0.8401 validation accuracy: 0.8172\n",
      "elapsed time : 137.2s\n",
      "tranning epoch 31: the SNN loss is 1.896868 trainning accuracy: 0.8352 validation accuracy: 0.7985\n",
      "elapsed time : 141.8s\n",
      "tranning epoch 32: the SNN loss is 1.927085 trainning accuracy: 0.8362 validation accuracy: 0.7892\n",
      "elapsed time : 146.5s\n",
      "tranning epoch 33: the SNN loss is 1.906682 trainning accuracy: 0.8313 validation accuracy: 0.8302\n",
      "elapsed time : 151.0s\n",
      "tranning epoch 34: the SNN loss is 1.897994 trainning accuracy: 0.8363 validation accuracy: 0.8191\n",
      "elapsed time : 155.6s\n",
      "tranning epoch 35: the SNN loss is 1.895946 trainning accuracy: 0.8347 validation accuracy: 0.8411\n",
      "elapsed time : 159.9s\n",
      "tranning epoch 36: the SNN loss is 1.902013 trainning accuracy: 0.8407 validation accuracy: 0.8248\n",
      "elapsed time : 164.2s\n",
      "tranning epoch 37: the SNN loss is 1.927576 trainning accuracy: 0.8477 validation accuracy: 0.8411\n",
      "elapsed time : 168.5s\n",
      "tranning epoch 38: the SNN loss is 1.917089 trainning accuracy: 0.8427 validation accuracy: 0.8196\n",
      "elapsed time : 172.9s\n",
      "tranning epoch 39: the SNN loss is 1.920633 trainning accuracy: 0.8424 validation accuracy: 0.7549\n",
      "elapsed time : 177.2s\n",
      "tranning epoch 40: the SNN loss is 1.901766 trainning accuracy: 0.8526 validation accuracy: 0.8450\n",
      "elapsed time : 181.4s\n",
      "tranning epoch 41: the SNN loss is 1.891184 trainning accuracy: 0.8513 validation accuracy: 0.8331\n",
      "elapsed time : 185.8s\n",
      "tranning epoch 42: the SNN loss is 1.918049 trainning accuracy: 0.8530 validation accuracy: 0.8337\n",
      "elapsed time : 190.1s\n",
      "tranning epoch 43: the SNN loss is 1.903548 trainning accuracy: 0.8527 validation accuracy: 0.8489\n",
      "elapsed time : 194.5s\n"
     ]
    }
   ],
   "source": [
    "# load dataset (trainset과 testset을 8 : 2 비율로 분리)\n",
    "#dataset = SNN_Dataset(data_path)\n",
    "#dataset_size = len(dataset)\n",
    "#trainset, testset = random_split(dataset, [dataset_size - dataset_size // 5, dataset_size // 5])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=param['data_dir'], train=True,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST(root=param['data_dir'], train=False,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset, batch_size=param['batch_size'],\n",
    "                                            shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset, batch_size=param['batch_size'],\n",
    "                                            shuffle=False, drop_last=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "#Train the SNN with BP\n",
    "net = Three_Layer_SNN(param).to(device)\n",
    "loss_func = torch.nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adam(net.parameters(), param['learning_rate'])\n",
    "# I have tried Adam and SGD, but only Adam performs well with my Gquant_ function\n",
    "# Before you use Gquant_, you should debug to figure out a proper lr, because too small lr\n",
    "# will induce too small delta w (smaller than the quantization error), then your net parameter\n",
    "# will get no change.\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(param['epoch']):\n",
    "    net.train()\n",
    "    train_accuracy = []\n",
    "    \n",
    "    for img, label in trainloader:\n",
    "        img = img.reshape(-1, input_dimmension)\n",
    "        spike_num_img = torch.zeros(param['batch_size'], param['dim_out']).to(device)\n",
    "\n",
    "        # using gpu\n",
    "        img, label = img.to(device), label.to(device)\n",
    "    \n",
    "        # rate coding method\n",
    "        #spike_time = rate_encoding(img, param['T_sim'])\n",
    "        \n",
    "        # Time-to-first spike coding method\n",
    "        #spike_time = latency(img, num_steps=param['T_sim'], tau=10, threshold=0.01, clip=False, normalize=False, linear=False, bypass=True)\n",
    "        spike_time = ttfs_encoding(img, param['T_sim'])\n",
    "        mask = torch.ones(param['batch_size'], 1).to(device)\n",
    "        \n",
    "        net.reset_() #set the neuron voltage as reset voltage\n",
    "        for t in range(param['T_sim']):\n",
    "            # Possion encoding method\n",
    "            # new_img = Poisson_encoder(img)\n",
    "            # out_spike = net(new_img)\n",
    "            # spike_num_img += out_spike \n",
    "            \n",
    "            # rate coding method\n",
    "            # new_img = spike_time[:, :, t]  \n",
    "            # out_spike = net(new_img)\n",
    "            # spike_num_img += out_spike \n",
    "            \n",
    "            # Time-to-first spike coding method\n",
    "             #new_img = spike_time[t]\n",
    "             new_img = spike_time[:, :, t]  \n",
    "             out_spike = net(new_img) * mask\n",
    "             spike_num_img += out_spike / pow(2, t)\n",
    "             # update mask\n",
    "             spike_detected = torch.any(out_spike > 0, dim=1, keepdim=True) \n",
    "             mask = mask * (~spike_detected) \n",
    "\n",
    "        #spike_rate = spike_num_img/param['T_sim'] \n",
    "        spike_rate = spike_num_img\n",
    "        loss = loss_func(spike_rate, label)\n",
    "\n",
    "        #net.zero_grad()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.reset_()    # reset the neuron voltage every batch, to ensure independency between batchs\n",
    "            net.quant_()    # quantize the weights after weight update\n",
    "\n",
    "        train_accuracy.append((spike_rate.max(1)[1] == label).float().mean().item())\n",
    "    accuracy_epoch = np.mean(train_accuracy)\n",
    "    print('tranning epoch %d: the SNN loss is %.6f' %(epoch, loss), end=' ')\n",
    "    print('trainning accuracy: %.4f' %accuracy_epoch, end=' ')\n",
    "\n",
    "    \n",
    "# validation by testset every epoch to see if the network is overfitted\n",
    "    net.eval()\n",
    "    validation_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for img_test, label_test in testloader:\n",
    "            img_test = img_test.reshape(-1, input_dimmension)\n",
    "            spike_num_img_test = torch.zeros(param['batch_size'], param['dim_out']).to(device)\n",
    "\n",
    "            # using gpu\n",
    "            img_test, label_test = img_test.to(device), label_test.to(device)\n",
    "\n",
    "            # rate coding method\n",
    "            #spike_time = rate_encoding(img_test, param['T_sim'])\n",
    "        \n",
    "            # Time-to-first spike coding method\n",
    "            #spike_time = latency(img_test, num_steps=param['T_sim'], tau=10, threshold=0.01, clip=False, normalize=False, linear=False, bypass = True)\n",
    "            spike_time = ttfs_encoding(img_test, param['T_sim'])\n",
    "            mask = torch.ones(param['batch_size'], 1).to(device)\n",
    "            \n",
    "            net.reset_() #set the neuron voltage as reset voltage\n",
    "            for t in range(param['T_sim']):\n",
    "                # Possion encoding method\n",
    "                # new_test_img = Poisson_encoder(img_test)\n",
    "                # out_spike = net(new_test_img)\n",
    "                # spike_num_img_test += out_spike \n",
    "\n",
    "                # rate coding method\n",
    "                # new_test_img = spike_time[:, :, t]  \n",
    "                # out_spike = net(new_test_img)\n",
    "                # spike_num_img_test += out_spike \n",
    "            \n",
    "                # Time-to-first spike coding method\n",
    "                # new_test_img = spike_time[t]\n",
    "                new_test_img = spike_time[:, :, t]\n",
    "                out_spike = net(new_test_img) * mask\n",
    "                spike_num_img_test += out_spike / pow(2, t)\n",
    "                # update mask\n",
    "                spike_detected = torch.any(out_spike > 0, dim=1, keepdim=True) \n",
    "                mask = mask * (~spike_detected) \n",
    "            \n",
    "            validation_accuracy.append((spike_num_img_test.max(1)[1]==label_test).float().mean().item())\n",
    "        accuracy_val = np.mean(validation_accuracy)\n",
    "        print('validation accuracy: %.4f' %accuracy_val)\n",
    "        print('elapsed time : {0:.1f}s' .format(time.time() - start))\n",
    "\n",
    "    with open(param['train_file'], 'a') as f_t:\n",
    "        s = str(epoch).ljust(6,' ') + str(round(loss.item(), 6)).ljust(12,' ')\n",
    "        s += str(round(accuracy_epoch, 4)).ljust(10, ' ') + str(round(accuracy_val, 4)).ljust(10, ' ') + '\\n'\n",
    "        f_t.write(s)\n",
    "\n",
    "torch.save(net.state_dict(), param['model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test process after training\n",
    "net_test = Three_Layer_SNN(param).to(device)\n",
    "print('Loading Model, please wait......')\n",
    "net_test.load_state_dict(torch.load(param['model_dir'], weights_only=True))\n",
    "print('Model loaded successfully!')\n",
    "list_num_spike = []\n",
    "for i in range(10):\n",
    "    list_num_spike.append([0])\n",
    "    list_num_spike[i].append(torch.zeros(param['dim_out']))\n",
    "\n",
    "total_correct = 0  # 전체 맞춘 예측 수\n",
    "total_samples = 0  # 전체 테스트 샘플 수\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img_test, label_test in testloader:\n",
    "        img_test = img_test.reshape(-1, 28 * 28)\n",
    "        spike_num_img_test = torch.zeros(param['batch_size'], param['dim_out']).to(device)\n",
    "        net_test.reset_()  # set the neuron voltage as reset voltage\n",
    "\n",
    "        # using gpu\n",
    "        img_test, label_test = img_test.to(device), label_test.to(device)\n",
    "\n",
    "        # Time-to-first spike coding method\n",
    "        #spike_time = latency(img_test, num_steps=param['T_sim'], tau=10, threshold=0.01, clip=False, normalize=False, linear=False, bypass=True)\n",
    "        spike_time = ttfs_encoding(img_test, param['T_sim'])\n",
    "        mask = torch.ones(param['batch_size'], 1).to(device)\n",
    "        net.reset_()  # set the neuron voltage as reset voltage\n",
    "\n",
    "        for t in range(param['T_sim']):\n",
    "            #new_test_img = spike_time[t]\n",
    "            new_test_img = spike_time[:, :, t]\n",
    "            out_spike = net_test(new_test_img) * mask\n",
    "            spike_num_img_test += out_spike / pow(2, t)\n",
    "            # update mask\n",
    "            spike_detected = torch.any(out_spike > 0, dim=1, keepdim=True)\n",
    "            mask = mask * (~spike_detected)\n",
    "\n",
    "        pred_label = F.one_hot(spike_num_img_test.max(1)[1], num_classes=10).to('cpu')  # convert the max neuron output index to onehot vector\n",
    "\n",
    "        # Accuracy calculation\n",
    "        pred_label_class = spike_num_img_test.max(1)[1]  # predicted class for each image in the batch\n",
    "        total_correct += (pred_label_class == label_test).sum().item()  # count correct predictions\n",
    "        total_samples += label_test.size(0)  # update total samples\n",
    "\n",
    "        for j in range(label_test.size(0)):\n",
    "            index = label_test[j]\n",
    "            list_num_spike[index][0] += 1\n",
    "            list_num_spike[index][1] += pred_label[j].to('cpu')  # statistics of prediction for every input image\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "accuracy = total_correct / total_samples * 100\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save confusion matrix\n",
    "with open('confusion_matrix.txt', 'a') as f2:\n",
    "    for i in range(len(list_num_spike)):\n",
    "        s = str(list_num_spike[i][0]) + ' ' + str(list_num_spike[i][1].numpy()).replace('[', '').replace(']', '') + '\\n'\n",
    "        f2.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
